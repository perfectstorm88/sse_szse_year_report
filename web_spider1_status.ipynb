{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook web_spider1_status.ipynb to script\n",
      "[NbConvertApp] Writing 2050 bytes to web_spider1_status.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n从新浪财经上下载年报：\\n\\n优先==》纯html，下载速度快\\n缺点==》只有2008年后的数据，并且有的公司还没有，数据不是太全\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!jupyter nbconvert --to script web_spider1_status.ipynb\n",
    "\"\"\"\n",
    "从新浪财经上下载年报：\n",
    "\n",
    "优先==》纯html，下载速度快\n",
    "缺点==》只有2008年后的数据，并且有的公司还没有，数据不是太全\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "pdf_needs = {}\n",
    "with open('./pdf_need.json') as f:\n",
    "    pdf_needs = json.load(f) # {'123456_2001':[pdf,txt,http1,http2,http3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查pdf文件和文本状态，把url进行合并\n",
    "\n",
    "import os\n",
    "pdf_dir = '../pdf/'\n",
    "txt_dir = '../txt/'\n",
    "def getSize(txt_filename):\n",
    "    fsize=0\n",
    "    try:\n",
    "        if os.path.exists(txt_filename):\n",
    "            fsize = os.path.getsize(txt_filename)\n",
    "    except Exception as e:\n",
    "        print('e',e)\n",
    "    return fsize\n",
    "\n",
    "for code_year,data in pdf_needs.items():\n",
    "    code,year = code_year.split('_')\n",
    "    while len(data)<2:\n",
    "        data.append(0)\n",
    "    pdf_file = os.path.join(pdf_dir,year,code_year+'.pdf')\n",
    "    data[0]=int(getSize(pdf_file)/1024)\n",
    "    txt_file = os.path.join(txt_dir,year,code_year+'.txt')\n",
    "    data[1]=int(getSize(txt_file)/1024)\n",
    "\n",
    "with open('pdf_status.json','w') as json_file:  \n",
    "    json.dump(pdf_needs,json_file, sort_keys=True, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_pdf_urls= 34916 ok_url_num= 30512 pdf_needs= 30555\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_pdf_urls = {}\n",
    "with open('./pdf_urls_sse_szse.json') as f:\n",
    "    all_pdf_urls = json.load(f)\n",
    "\n",
    "pdf_urls = {}\n",
    "url_num = 0\n",
    "for k,url in all_pdf_urls.items():\n",
    "    if k in pdf_needs:    \n",
    "        data = pdf_needs[k]\n",
    "        url_num +=1\n",
    "        if url not in data:\n",
    "            data.append(url)\n",
    "\n",
    "print('all_pdf_urls=',len(all_pdf_urls.keys()),'ok_url_num=',url_num,'pdf_needs=',len(pdf_needs.keys()))\n",
    "\n",
    "with open('pdf_status_with_url.json','w') as json_file:  \n",
    "    json.dump(pdf_needs,json_file, sort_keys=True, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf_hist {3: 753, 4: 1294, 5: 1784, 12: 445, 11: 497, 15: 372, 47: 90, 22: 822, 37: 214, 35: 281, 10: 654, 2: 1349, 7: 1513, 9: 884, 13: 417, 16: 437, 18: 531, 58: 37, 50: 65, 70: 15, 170: 5, 17: 450, 19: 561, 34: 321, 33: 398, 20: 550, 21: 673, 14: 363, 6: 1928, 23: 928, 27: 795, 29: 635, 24: 951, 8: 1133, 38: 215, 49: 83, 25: 957, 83: 12, 76: 6, 93: 5, 26: 924, 40: 146, 80: 9, 87: 7, 36: 242, 39: 158, 28: 755, 31: 505, 30: 567, 105: 4, 0: 1404, 69: 16, 44: 117, 60: 32, 52: 45, 45: 89, 1: 322, 63: 19, 56: 31, 245: 1, 54: 48, 89: 3, 98: 4, 97: 7, 77: 21, 59: 36, 42: 145, 46: 105, 57: 33, 101: 3, 43: 129, 67: 15, 48: 74, 75: 11, 86: 13, 41: 118, 61: 38, 88: 6, 162: 3, 78: 9, 32: 433, 94: 5, 632: 1, 51: 52, 136: 4, 73: 17, 53: 37, 62: 24, 141: 3, 55: 37, 108: 1, 90: 7, 68: 14, 66: 15, 241: 1, 71: 15, 134: 1, 191: 1, 156: 1, 126: 2, 65: 23, 72: 16, 81: 9, 137: 3, 64: 11, 151: 1, 95: 6, 186: 1, 110: 8, 102: 6, 82: 4, 109: 4, 206: 2, 91: 6, 99: 2, 114: 3, 138: 1, 142: 2, 153: 1, 160: 2, 165: 2, 171: 2, 172: 2, 180: 1, 149: 2, 74: 11, 266: 1, 207: 1, 198: 2, 107: 1, 100: 6, 84: 5, 152: 1, 79: 8, 201: 1, 104: 4, 146: 2, 158: 2, 193: 1, 178: 1, 135: 3, 92: 5, 111: 1, 192: 1, 203: 1, 130: 1, 164: 2, 96: 7, 127: 2, 120: 2, 249: 1, 139: 1, 122: 1, 112: 1, 124: 1, 103: 4, 117: 1, 106: 4, 125: 2, 634: 1, 273: 1, 177: 1, 176: 1, 115: 3, 132: 2, 196: 1, 211: 1, 354: 1, 131: 1, 283: 1, 133: 1, 166: 1, 382: 1, 509: 1, 271: 1, 236: 1, 234: 1, 119: 1, 85: 4, 179: 1, 286: 1, 148: 1, 200: 1, 277: 1, 116: 1, 129: 1, 155: 1, 118: 1, 502: 1, 167: 1, 183: 1, 123: 1}\n",
      "txt_hist {4: 3795, 6: 4595, 7: 2865, 8: 1714, 9: 1041, 10: 729, 0: 4612, 13: 207, 11: 485, 12: 345, 14: 177, 17: 9, 2: 1637, 3: 3043, 5: 5141, 15: 82, 23: 3, 22: 2, 32: 1, 34: 1, 16: 16, 1: 40, 20: 4, 21: 4, 18: 2, 24: 3, 41: 1, 19: 1}\n",
      "pdf_size_zero= 1404\n",
      "txt_size_zero= 4612\n"
     ]
    }
   ],
   "source": [
    "def count_elements(seq) -> dict:\n",
    "    \"\"\"Tally elements from `seq`.\"\"\"\n",
    "    hist = {}\n",
    "    for i in seq:\n",
    "        hist[i] = hist.get(i, 0) + 1\n",
    "    return hist\n",
    "pdf_hist = count_elements([int(x[0]/100) for x in pdf_needs.values()])\n",
    "print(\"pdf_hist\",pdf_hist)\n",
    "txt_hist = count_elements([int(x[1]/50) for x in pdf_needs.values()])\n",
    "print(\"txt_hist\",txt_hist)\n",
    "print('pdf_size_zero=',pdf_hist.get(0))\n",
    "print('txt_size_zero=',txt_hist.get(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
