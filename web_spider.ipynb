{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import bs4\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pdb\n",
    "import pickle\n",
    "\n",
    "url_prefix = 'http://vip.stock.finance.sina.com.cn'\n",
    "url_pattern = r'href=\".+target='\n",
    "title_pattern = r'_blank\">.+</a>'\n",
    "year_pattern = r'20[01][0-9]'\n",
    "\n",
    "code = '000001'\n",
    "\n",
    "def getTitle(code):\n",
    "    try:\n",
    "        reports ={}\n",
    "        company_url = f'http://vip.stock.finance.sina.com.cn/corp/go.php/vCB_Bulletin/stockid/{code}/page_type/ndbg.phtml'\n",
    "        print(f'Processing {code} ')\n",
    "\n",
    "        page_client = urlopen(company_url)\n",
    "        page = page_client.read().decode('gbk')\n",
    "        page_client.close()\n",
    "\n",
    "        page_soup = soup(page, \"html.parser\")\n",
    "        content = page_soup.find('div',{'class':'datelist'})\n",
    "\n",
    "        all_url_title = content.findAll('a')\n",
    "\n",
    "        current_id_set = set()\n",
    "        for v in all_url_title:\n",
    "            url = url_prefix + re.findall(url_pattern,str(v))[0][6:-9]\n",
    "            url = url.replace('amp;','')          \n",
    "\n",
    "            title = re.findall(title_pattern,str(v))[0][8:-4]\n",
    "            year = re.findall(year_pattern, title)\n",
    "            if not year: continue\n",
    "            year = year[0]\n",
    "            report_id = code+'_'+year\n",
    "            if report_id not in reports:\n",
    "                reports[report_id]=url;\n",
    "                \n",
    "#         print(reports)\n",
    "        return reports\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "getTitle(code)  \n",
    "\n",
    "def readContent(url):\n",
    "    try:\n",
    "        page_client = urlopen(url)\n",
    "        page = page_client.read().decode('gbk')\n",
    "        page_client.close()\n",
    "\n",
    "        report_soup = soup(page, \"html.parser\", )\n",
    "\n",
    "        content = str(report_soup.find('pre'))\n",
    "#         print(content)\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# url = 'http://vip.stock.finance.sina.com.cn/corp/view/vCB_AllBulletinDetail.php?stockid=000004&id=4386458'; \n",
    "# readContent(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "df1 = pd.read_excel('out_join2_20190318.xlsx',index_col=0,\n",
    "            dtype={'Stkcd': str})\n",
    "df1.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "col=df1.iloc[:,0]\n",
    "#取表中的第3列的所有值\n",
    "arrs=col.values\n",
    "#输出结果\n",
    "ids = list(set(arrs))\n",
    "import os\n",
    "import time\n",
    "counter=0\n",
    "for i in tqdm(ids):\n",
    "    reports = getTitle(i)\n",
    "    retry = 0\n",
    "    while not reports and retry<3:\n",
    "        print(f\"获取年报地址失败:{i}, retrying..\")\n",
    "        time.sleep(1+1*retry*retry)\n",
    "        reports = getTitle(i)\n",
    "        retry += 1\n",
    "\n",
    "    if not reports:\n",
    "        print(f\"获取年报地址失败:{i}\",'*'*50)\n",
    "        with open('error.txt','a') as f2:\n",
    "            f2.write(f'{i}\\n')\n",
    "            f2.close()\n",
    "        continue\n",
    "    for (f,url) in reports.items():\n",
    "        file = f\"scraping/content/{f}.txt\"\n",
    "        if os.path.exists(file):\n",
    "            continue\n",
    "        content = readContent(url)\n",
    "        retry = 0\n",
    "        while not content and retry<3:\n",
    "            print(f\"获取年报地址失败:{i}, retrying..\")\n",
    "            time.sleep(0.1+0.5*retry*retry)\n",
    "            content = readContent(url)\n",
    "            retry += 1\n",
    "        if not content:\n",
    "            print( f\"下载文件失败:{url}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"下载文件成功:{url}\")\n",
    "        with open(file,'w') as f:\n",
    "            counter += 1\n",
    "            f.write(content)\n",
    "        \n",
    "        if counter %100 ==0:\n",
    "            print('*'*80)\n",
    "            print('下载文件数',counter)\n",
    "            print('*'*80)\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1[['Stkcd','Reptdt','Stknme']] \n",
    "df2\n",
    "data = df2.to_dict('records')\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
